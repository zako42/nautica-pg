{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nautica-pg-lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHyyeC0CjSZAqCZc/9DHEZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zako42/nautica-pg/blob/main/nautica_pg_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUhpAKxtaG9a"
      },
      "source": [
        "Code below is from OpenAI \"Spinning Up\" tutorial:\n",
        "\n",
        "https://spinningup.openai.com/en/latest/index.html\n",
        "\n",
        "Go through the background information first:\n",
        "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
        "\n",
        "We wont go into part 2, but will look at part 3:\n",
        "https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
        "\n",
        "This includes some RL background info, and going thru the code for REINFORCE, and then your homework will be to add \"advantage\" to it, going from REINFORCE -> Vanilla PG\n",
        "\n",
        "Section below is the code from the vanilla policy gradient \"the simplest equation describing the gradient of policy performance with respect to policy parameters\" (this is the REINFORCE algorithm)\n",
        "\n",
        "https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL3T2zTOZx29"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r80vZKcfbrat"
      },
      "source": [
        "After the imports, the method below dynamically sets up a fully connected (Linear) MLP, using sizes passed in. This uses tanh for neurons activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReJxNNfObxGA"
      },
      "source": [
        "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
        "    # Build a feedforward neural network.\n",
        "    # note it's a fully connected, with however many layers\n",
        "    # we take the \n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nm9dKFbb2W4"
      },
      "source": [
        "The \"simplest\" code below, doesn't use advantage. It does just the policy gradient part. You can add in the advantage for homework. The full code for Vanilla PG (with advantage) is here:  https://github.com/jachiam/rl-intro/blob/master/pg_cartpole.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDDuBuudb3DT"
      },
      "source": [
        "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
        "          epochs=50, batch_size=5000, render=False):\n",
        "\n",
        "    # make environment, check spaces, get obs / act dims\n",
        "    env = gym.make(env_name)\n",
        "    assert isinstance(env.observation_space, Box), \\\n",
        "        \"This example only works for envs with continuous state spaces.\"\n",
        "    assert isinstance(env.action_space, Discrete), \\\n",
        "        \"This example only works for envs with discrete action spaces.\"\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.n\n",
        "\n",
        "    # make core of policy network\n",
        "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
        "\n",
        "    # make function to compute action distribution\n",
        "    # (does the forward pass on the NN)\n",
        "    def get_policy(obs):\n",
        "        logits = logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    # make action selection function (outputs int actions, sampled from policy)\n",
        "    # note this gets an actual action by sampling from the distribution\n",
        "    def get_action(obs):\n",
        "        return get_policy(obs).sample().item()\n",
        "\n",
        "    # make loss function whose gradient, for the right data, is policy gradient\n",
        "    def compute_loss(obs, act, weights):\n",
        "        logp = get_policy(obs).log_prob(act)\n",
        "        return -(logp * weights).mean()\n",
        "\n",
        "    # make optimizer\n",
        "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
        "\n",
        "    # for training policy\n",
        "    def train_one_epoch():\n",
        "        # make some empty lists for logging.\n",
        "        batch_obs = []          # for observations\n",
        "        batch_acts = []         # for actions\n",
        "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
        "        batch_rets = []         # for measuring episode returns\n",
        "        batch_lens = []         # for measuring episode lengths\n",
        "\n",
        "        # reset episode-specific variables\n",
        "        obs = env.reset()       # first obs comes from starting distribution\n",
        "        done = False            # signal from environment that episode is over\n",
        "        ep_rews = []            # list for rewards accrued throughout ep\n",
        "\n",
        "        # render first episode of each epoch\n",
        "        finished_rendering_this_epoch = False\n",
        "\n",
        "        # collect experience by acting in the environment with current policy\n",
        "        # Notice below, almost all the code is for running episodes, and collecting the states, actions, rewards\n",
        "        while True:\n",
        "\n",
        "            # rendering\n",
        "            if (not finished_rendering_this_epoch) and render:\n",
        "                env.render()\n",
        "\n",
        "            # save obs\n",
        "            batch_obs.append(obs.copy())\n",
        "\n",
        "            # act in the environment\n",
        "            # calls the methods defined above to get an actual action from the policy\n",
        "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "            obs, rew, done, _ = env.step(act)\n",
        "\n",
        "            # save action, reward\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "\n",
        "            if done:\n",
        "                # if episode is over, record info about episode\n",
        "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "                batch_rets.append(ep_ret)\n",
        "                batch_lens.append(ep_len)\n",
        "\n",
        "                # the weight for each logprob(a|s) is R(tau)\n",
        "                batch_weights += [ep_ret] * ep_len\n",
        "\n",
        "                # reset episode-specific variables\n",
        "                obs, done, ep_rews = env.reset(), False, []\n",
        "\n",
        "                # won't render again this epoch\n",
        "                finished_rendering_this_epoch = True\n",
        "\n",
        "                # end experience loop if we have enough of it\n",
        "                if len(batch_obs) > batch_size:\n",
        "                    break\n",
        "\n",
        "        # take a single policy gradient update step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
        "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
        "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
        "                                  )\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
        "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR6MFk-Ib83j"
      },
      "source": [
        "In the code from the link, it has the main() but causes error in the notebook, so I removed it. They have it set up with default args, so we can just call train() below, and it will use CartPole v0 (see train() default args above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9s2S1UHcFoz",
        "outputId": "64e6bc43-0f0d-4ed8-8ffa-af82b318d3b5"
      },
      "source": [
        "train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   0 \t loss: 18.784 \t return: 21.586 \t ep_len: 21.586\n",
            "epoch:   1 \t loss: 19.365 \t return: 22.719 \t ep_len: 22.719\n",
            "epoch:   2 \t loss: 25.177 \t return: 25.431 \t ep_len: 25.431\n",
            "epoch:   3 \t loss: 25.423 \t return: 29.057 \t ep_len: 29.057\n",
            "epoch:   4 \t loss: 27.824 \t return: 31.604 \t ep_len: 31.604\n",
            "epoch:   5 \t loss: 30.288 \t return: 34.854 \t ep_len: 34.854\n",
            "epoch:   6 \t loss: 37.806 \t return: 39.945 \t ep_len: 39.945\n",
            "epoch:   7 \t loss: 34.039 \t return: 41.106 \t ep_len: 41.106\n",
            "epoch:   8 \t loss: 37.616 \t return: 44.257 \t ep_len: 44.257\n",
            "epoch:   9 \t loss: 45.801 \t return: 51.296 \t ep_len: 51.296\n",
            "epoch:  10 \t loss: 46.574 \t return: 56.843 \t ep_len: 56.843\n",
            "epoch:  11 \t loss: 50.739 \t return: 56.494 \t ep_len: 56.494\n",
            "epoch:  12 \t loss: 48.301 \t return: 61.171 \t ep_len: 61.171\n",
            "epoch:  13 \t loss: 59.522 \t return: 70.746 \t ep_len: 70.746\n",
            "epoch:  14 \t loss: 62.610 \t return: 73.087 \t ep_len: 73.087\n",
            "epoch:  15 \t loss: 59.381 \t return: 75.194 \t ep_len: 75.194\n",
            "epoch:  16 \t loss: 69.299 \t return: 86.483 \t ep_len: 86.483\n",
            "epoch:  17 \t loss: 74.196 \t return: 98.804 \t ep_len: 98.804\n",
            "epoch:  18 \t loss: 73.860 \t return: 95.037 \t ep_len: 95.037\n",
            "epoch:  19 \t loss: 82.508 \t return: 113.867 \t ep_len: 113.867\n",
            "epoch:  20 \t loss: 83.465 \t return: 116.628 \t ep_len: 116.628\n",
            "epoch:  21 \t loss: 95.483 \t return: 145.457 \t ep_len: 145.457\n",
            "epoch:  22 \t loss: 86.064 \t return: 121.952 \t ep_len: 121.952\n",
            "epoch:  23 \t loss: 93.012 \t return: 147.441 \t ep_len: 147.441\n",
            "epoch:  24 \t loss: 94.406 \t return: 150.912 \t ep_len: 150.912\n",
            "epoch:  25 \t loss: 97.007 \t return: 152.824 \t ep_len: 152.824\n",
            "epoch:  26 \t loss: 107.045 \t return: 177.793 \t ep_len: 177.793\n",
            "epoch:  27 \t loss: 104.540 \t return: 170.433 \t ep_len: 170.433\n",
            "epoch:  28 \t loss: 97.567 \t return: 154.576 \t ep_len: 154.576\n",
            "epoch:  29 \t loss: 94.162 \t return: 156.312 \t ep_len: 156.312\n",
            "epoch:  30 \t loss: 97.071 \t return: 154.788 \t ep_len: 154.788\n",
            "epoch:  31 \t loss: 96.473 \t return: 154.242 \t ep_len: 154.242\n",
            "epoch:  32 \t loss: 99.977 \t return: 165.355 \t ep_len: 165.355\n",
            "epoch:  33 \t loss: 95.686 \t return: 159.094 \t ep_len: 159.094\n",
            "epoch:  34 \t loss: 98.572 \t return: 162.419 \t ep_len: 162.419\n",
            "epoch:  35 \t loss: 103.913 \t return: 171.967 \t ep_len: 171.967\n",
            "epoch:  36 \t loss: 98.806 \t return: 159.719 \t ep_len: 159.719\n",
            "epoch:  37 \t loss: 102.935 \t return: 173.103 \t ep_len: 173.103\n",
            "epoch:  38 \t loss: 99.298 \t return: 162.774 \t ep_len: 162.774\n",
            "epoch:  39 \t loss: 104.848 \t return: 178.931 \t ep_len: 178.931\n",
            "epoch:  40 \t loss: 107.650 \t return: 182.464 \t ep_len: 182.464\n",
            "epoch:  41 \t loss: 108.825 \t return: 184.714 \t ep_len: 184.714\n",
            "epoch:  42 \t loss: 109.055 \t return: 186.222 \t ep_len: 186.222\n",
            "epoch:  43 \t loss: 103.222 \t return: 171.333 \t ep_len: 171.333\n",
            "epoch:  44 \t loss: 100.240 \t return: 167.867 \t ep_len: 167.867\n",
            "epoch:  45 \t loss: 96.390 \t return: 164.645 \t ep_len: 164.645\n",
            "epoch:  46 \t loss: 96.797 \t return: 154.727 \t ep_len: 154.727\n",
            "epoch:  47 \t loss: 100.680 \t return: 172.033 \t ep_len: 172.033\n",
            "epoch:  48 \t loss: 101.172 \t return: 172.400 \t ep_len: 172.400\n",
            "epoch:  49 \t loss: 110.120 \t return: 190.333 \t ep_len: 190.333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcUksf9ke5dP"
      },
      "source": [
        "Below is the Vanilla PG code from the spinning up tutorial: https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyI_ksp7fywr"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def mlp(x, hidden_sizes=(32,32), activation=tf.tanh):\n",
        "    for size in hidden_sizes:\n",
        "        x = tf.layers.dense(x, units=size, activation=activation)\n",
        "    return x\n",
        "\n",
        "def discount_cumsum(x, gamma):\n",
        "    n = len(x)\n",
        "    x = np.array(x)\n",
        "    y = gamma**np.arange(n)\n",
        "    z = np.zeros_like(x, dtype=np.float32)\n",
        "    for j in range(n):\n",
        "        z[j] = sum(x[j:] * y[:n-j])\n",
        "    return z\n",
        "\n",
        "def train(env_name='CartPole-v0', hidden_dim=32, n_layers=1,\n",
        "          lr=1e-2, gamma=0.99, n_iters=50, batch_size=5000\n",
        "          ):\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.n\n",
        "\n",
        "    # make model\n",
        "    with tf.variable_scope('model'):\n",
        "        obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
        "        net = mlp(obs_ph, hidden_sizes=[hidden_dim]*n_layers)\n",
        "        logits = tf.layers.dense(net, units=n_acts, activation=None)\n",
        "        actions = tf.squeeze(tf.multinomial(logits=logits,num_samples=1), axis=1)\n",
        "\n",
        "    # make loss\n",
        "    adv_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
        "    act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
        "    action_one_hots = tf.one_hot(act_ph, n_acts)\n",
        "    log_probs = tf.reduce_sum(action_one_hots * tf.nn.log_softmax(logits), axis=1)\n",
        "    loss = -tf.reduce_mean(adv_ph * log_probs)\n",
        "\n",
        "    # make train op\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # train model\n",
        "    def train_one_iteration():\n",
        "        batch_obs, batch_acts, batch_rtgs, batch_rets, batch_lens = [], [], [], [], []\n",
        "\n",
        "        obs, rew, done, ep_rews = env.reset(), 0, False, []\n",
        "        while True:\n",
        "            batch_obs.append(obs.copy())\n",
        "            act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]\n",
        "            obs, rew, done, _ = env.step(act)\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "            if done:\n",
        "                batch_rets.append(sum(ep_rews))\n",
        "                batch_lens.append(len(ep_rews))\n",
        "                batch_rtgs += list(discount_cumsum(ep_rews, gamma))\n",
        "                obs, rew, done, ep_rews = env.reset(), 0, False, []\n",
        "                if len(batch_obs) > batch_size:\n",
        "                    break\n",
        "\n",
        "        # normalize advs trick:\n",
        "        batch_advs = np.array(batch_rtgs)\n",
        "        batch_advs = (batch_advs - np.mean(batch_advs))/(np.std(batch_advs) + 1e-8)\n",
        "        batch_loss, _ = sess.run([loss, train_op], feed_dict={obs_ph: np.array(batch_obs),\n",
        "                                                              act_ph: np.array(batch_acts),\n",
        "                                                              adv_ph: batch_advs})\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_iteration()\n",
        "        print('itr: %d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
        "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg1TZwvGgIz1"
      },
      "source": [
        "The code above is using TF 1.x and I don't think it runs. But you can look at the code example to get the idea for adding advantage into the REINFORCE code earlier, to make it into VPG."
      ]
    }
  ]
}